# Custom PrometheusRule Alerts
# US-014: Configure Basic Alerts for cluster health monitoring
# Requires: kube-prometheus-stack deployed in observability namespace

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: custom-alerts
  namespace: observability
  labels:
    release: prometheus
    app: kube-prometheus-stack
spec:
  groups:
    - name: custom.rules
      rules:
        # =============================================================================
        # Disk Usage Alerts
        # =============================================================================
        - alert: DiskUsageHigh
          expr: |
            (1 - node_filesystem_avail_bytes{fstype!~"tmpfs|overlay",mountpoint!~"/run.*|/sys.*|/snap.*",device!=""}
            / node_filesystem_size_bytes{fstype!~"tmpfs|overlay",mountpoint!~"/run.*|/sys.*|/snap.*",device!=""}) * 100 > 80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Disk usage above 80%"
            description: "Filesystem {{ $labels.mountpoint }} on {{ $labels.instance }} is {{ printf \"%.1f\" $value }}% full."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/k8s/observability/custom-alerts-README.md#diskusagehigh"

        - alert: DiskUsageCritical
          expr: |
            (1 - node_filesystem_avail_bytes{fstype!~"tmpfs|overlay",mountpoint!~"/run.*|/sys.*|/snap.*",device!=""}
            / node_filesystem_size_bytes{fstype!~"tmpfs|overlay",mountpoint!~"/run.*|/sys.*|/snap.*",device!=""}) * 100 > 90
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Disk usage above 90% - CRITICAL"
            description: "Filesystem {{ $labels.mountpoint }} on {{ $labels.instance }} is {{ printf \"%.1f\" $value }}% full. Immediate action required."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/k8s/observability/custom-alerts-README.md#diskusagecritical"

        # =============================================================================
        # Memory Usage Alerts
        # =============================================================================
        - alert: MemoryUsageWarning
          expr: |
            (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Memory usage above 80%"
            description: "Node {{ $labels.instance }} memory usage is {{ printf \"%.1f\" $value }}%. Monitor for potential issues."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/k8s/observability/custom-alerts-README.md#memoryusagewarning"

        - alert: MemoryUsageHigh
          expr: |
            (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 90
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Memory usage above 90%"
            description: "Node {{ $labels.instance }} memory usage is {{ printf \"%.1f\" $value }}%. Consider scaling or investigating memory-hungry workloads."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/k8s/observability/custom-alerts-README.md#memoryusagehigh"

        # =============================================================================
        # Pod Health Alerts
        # =============================================================================
        - alert: PodCrashLooping
          expr: |
            increase(kube_pod_container_status_restarts_total[10m]) > 3
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: "Pod crash looping"
            description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} (container {{ $labels.container }}) has restarted {{ printf \"%.0f\" $value }} times in the last 10 minutes."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/k8s/observability/custom-alerts-README.md#podcrashlooping"

        # =============================================================================
        # Node Health Alerts
        # =============================================================================
        - alert: NodeNotReady
          expr: |
            kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Node not ready"
            description: "Node {{ $labels.node }} has been in NotReady state for more than 2 minutes."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/k8s/observability/custom-alerts-README.md#nodenotready"

        # =============================================================================
        # Database Alerts (US-026)
        # =============================================================================
        - alert: DatabasePoolExhausted
          expr: |
            db_pool_connections_waiting > 0
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Database connection pool exhausted"
            description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has {{ printf \"%.0f\" $value }} queries waiting for connections."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/docs/guides/troubleshooting.md#database-issues"

        - alert: DatabaseQuerySlow
          expr: |
            histogram_quantile(0.99, rate(db_query_duration_seconds_bucket[5m])) > 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Database queries are slow"
            description: "P99 query latency in {{ $labels.namespace }}/{{ $labels.pod }} is {{ printf \"%.2f\" $value }}s."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/docs/guides/troubleshooting.md#database-issues"

        # =============================================================================
        # Namespace Quota Alerts (US-026)
        # =============================================================================
        - alert: NamespaceQuotaCPUApproaching
          expr: |
            (kube_resourcequota{resource="limits.cpu", type="used"}
            / kube_resourcequota{resource="limits.cpu", type="hard"}) * 100 > 80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Namespace CPU quota approaching limit"
            description: "Namespace {{ $labels.namespace }} is using {{ printf \"%.0f\" $value }}% of CPU quota."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/docs/guides/troubleshooting.md#resource-issues"

        - alert: NamespaceQuotaMemoryApproaching
          expr: |
            (kube_resourcequota{resource="limits.memory", type="used"}
            / kube_resourcequota{resource="limits.memory", type="hard"}) * 100 > 80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Namespace memory quota approaching limit"
            description: "Namespace {{ $labels.namespace }} is using {{ printf \"%.0f\" $value }}% of memory quota."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/docs/guides/troubleshooting.md#resource-issues"

        # =============================================================================
        # API SLO Alerts (US-026)
        # =============================================================================
        - alert: APIHighLatency
          expr: |
            histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 0.5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "API latency SLO breach"
            description: "P99 latency for {{ $labels.route }} in {{ $labels.namespace }} is {{ printf \"%.3f\" $value }}s (SLO: 500ms)."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/docs/guides/troubleshooting.md#application-issues"

        - alert: APIHighErrorRate
          expr: |
            (sum by (namespace, route) (rate(http_requests_total{status_code=~"5.."}[5m]))
            / sum by (namespace, route) (rate(http_requests_total[5m]))) * 100 > 5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "API error rate high"
            description: "Error rate for {{ $labels.route }} in {{ $labels.namespace }} is {{ printf \"%.1f\" $value }}%."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/docs/guides/troubleshooting.md#application-issues"

        # =============================================================================
        # Prometheus Scrape Alerts (US-026)
        # =============================================================================
        - alert: PrometheusScrapeFailure
          expr: |
            up{job=~".*demo-app.*"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Prometheus scrape failing"
            description: "Cannot scrape metrics from {{ $labels.instance }} in {{ $labels.namespace }}."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/docs/guides/troubleshooting.md#observability-issues"

        - alert: PrometheusTargetMissing
          expr: |
            absent(up{job=~".*demo-app.*"})
          for: 10m
          labels:
            severity: info
          annotations:
            summary: "No demo-app targets found"
            description: "No demo-app ServiceMonitor targets are being scraped. This may be expected if no PR environments are active."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/docs/guides/troubleshooting.md#observability-issues"

        # =============================================================================
        # Loki Alerts (US-026)
        # =============================================================================
        - alert: LokiIngestionErrors
          expr: |
            rate(loki_distributor_bytes_received_total[5m]) == 0
            and
            sum(rate(container_network_transmit_bytes_total{namespace="observability", pod=~"loki.*"}[5m])) > 0
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Loki log ingestion stopped"
            description: "Loki is not receiving logs despite network activity. Check Promtail and Loki health."
            runbook_url: "https://github.com/koder-cat/k8s-ephemeral-environments/blob/main/docs/guides/troubleshooting.md#observability-issues"
